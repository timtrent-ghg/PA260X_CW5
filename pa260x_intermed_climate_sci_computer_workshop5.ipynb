{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2af570-8b4d-495d-b61d-e37cc1cbf033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import netCDF4\n",
    "from scipy import stats\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a609535-66f7-43c4-a2b0-5bc11acb1bc0",
   "metadata": {},
   "source": [
    "***\n",
    "# Functions for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9607d-ab97-4968-8654-946c9894aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#                                                                      #\n",
    "########################################################################\n",
    "def read_csv(filename):\n",
    "    \"\"\" read in the csv files we previously made of UTH anomaly time series.\n",
    "    inputs: filename -> full path to file containing our data\n",
    "\n",
    "    ouputs: results -> python dictionary containing our anomaly time series.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    data = np.loadtxt(filename, skiprows=1, delimiter=\",\",dtype='<f4')\n",
    "    header = np.genfromtxt(filename, delimiter=',', dtype=str, max_rows=1)\n",
    "    for ii, varname in enumerate(header):\n",
    "        results[varname] = data[:,ii]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6093ff3-9d65-4650-86b6-e87c7017fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#                                                                      #\n",
    "########################################################################\n",
    "def simple_moving_average(yvals, width):\n",
    "    \"\"\" compute the moving average of a time series with a user defined sliding window.\n",
    "    inputs: yvals -> time series on regular time steps\n",
    "            width -> width of sliding window (n time steps)\n",
    "    output: sommothed time series\n",
    "    \"\"\"\n",
    "    return np.convolve(yvals, np.ones(width), 'same')/width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705cfbfa-93fd-4396-8d7b-00dbe7b55c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#                                                                      #\n",
    "########################################################################\n",
    "def find_enso_periods(smoothed_ts,enso_idx,dates,nmonths,phase='Elnino'):\n",
    "\n",
    "    cnt, index, periods, max_anom = 0,[],[],[]\n",
    "    for ii, val in enumerate(enso_idx):\n",
    "        if val != 0:\n",
    "            cnt+=1\n",
    "            index.append(ii)\n",
    "        else:\n",
    "            if cnt == 0:\n",
    "                pass\n",
    "            else:\n",
    "                if cnt >=nmonths:\n",
    "                    periods.append(dates[index])\n",
    "                    if phase == 'Elnino':\n",
    "                        max_anom.append(max(smoothed_ts[index]))\n",
    "                    else:\n",
    "                        max_anom.append(min(smoothed_ts[index]))\n",
    "                cnt = 0\n",
    "                index = []\n",
    "    ####################################################################\n",
    "    years = []\n",
    "    for dd in periods:\n",
    "        st = f\"{int(dd[0])}\"\n",
    "        fn = f\"{int(dd[-1])}\"\n",
    "        years.append(f\"{st}/{fn}\")\n",
    "\n",
    "    return periods, max_anom, years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca865d-1357-4194-ac17-f7d99eecc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "def calculate_probDensFunc(yvals, ymin, ymax,nbins=100):\n",
    "    \"\"\" calculate a PDF from the array yvals using scipy.stats norm module\n",
    "    inputs: yvals -> 1d array of values to be used in PDF calculation\n",
    "            ymin  -> minimum value for range of yvals PDF is calculated\n",
    "            ymax  -> maximum value for range of yvals PDF is calculated\n",
    "            nbins -> number of bins for which PDF is calculated between ymin  and ymax\n",
    "            \n",
    "    outputs: xvals -> array of values over which the PDF was calculated (defined by ymin, ymax,nbins)\n",
    "             PDF   -> array containing PDF values\n",
    "    \"\"\"\n",
    "    # define xvals\n",
    "    xvals = np.linspace(ymin,ymax,nbins)\n",
    "\n",
    "    # calculate mean and standard deviation of yvals\n",
    "    mu = np.mean(yvals)\n",
    "    std = np.std(yvals)\n",
    "\n",
    "    # define an empty array to hold PDF values. Here we use the size method to tell the code how many elements \n",
    "    # are in this new array and fill each entry with a default value NaN (Not a Number)\n",
    "    PDF = np.full(xvals.size, np.nan)\n",
    "\n",
    "    # loop over each value in x and calculate the corresponding PDF value. Because xvals is an array merans in \n",
    "    # Python it is iterable (i.e. we can loop over the contents) and by wrapping it in the enumerate function\n",
    "    # we also get the index of the value (e.g. the firts value in xvals could be -2, therefore x=-2 and ii=0).\n",
    "    for ii, x in enumerate(xvals):\n",
    "        PDF[ii] = norm.pdf(x, loc=mu, scale=std)\n",
    "\n",
    "    # return the results\n",
    "    return PDF, xvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47e55c-d0df-46b7-bf4e-e029dae5b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_enso_phase_pdfs(xvals, el_pre_pdf, el_aft_pdf, la_pre_pdf, la_aft_pdf, vname):\n",
    "    \"\"\" plot pdf of either uth or lst\"\"\"\n",
    "    plt.plot(xvals, el_pre_pdf, \":\", lw=2,color='#1F3649',alpha=0.7,label='El Nino (1979-2000)')\n",
    "    plt.plot(xvals, la_pre_pdf, \":\",lw=2,color=\"#2E75B6\",alpha=0.7,label='La Nina (1979-2020)')\n",
    "    \n",
    "    plt.plot(xvals, el_aft_pdf,lw=2,color='#1F3649',label='El Nino (2001-2024)')\n",
    "    plt.plot(xvals, la_aft_pdf,lw=2,color=\"#2E75B6\",label='La Nina (2001-2024)')\n",
    "    if vname == \"lst\":\n",
    "        plt.xlim(-2,3)\n",
    "        plt.ylim(0,1.6)\n",
    "        plt.xlabel(r\"$\\Delta$LST [K]\",fontsize=12)\n",
    "    elif vname == 'uth':\n",
    "        plt.xlim(-3,2)\n",
    "        plt.ylim(0,1.5)\n",
    "        plt.xlabel(r\"$\\Delta$UTH [%]\",fontsize=12)\n",
    "    else:\n",
    "        pass\n",
    "    plt.ylabel(\"PDF\",fontsize=12)\n",
    "    plt.legend(loc=1,fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912cad3-69f5-4f1e-9f0d-8422c6556d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uth_vs_Ts(ts_anom, uth_anom, region1, region2):\n",
    "    #coeffs = estimate_coef(ts_anom, uth_anom)\n",
    "    slope, intercept, r, p, se = stats.linregress(ts_anom, uth_anom)\n",
    "    # calculate the pearson coeeficients\n",
    "    #R = stats.pearsonr(ts_anom, uth_anom)\n",
    "    # convert pearson coefficient to a string \n",
    "    Rs = f\"{r:0.2f}\"\n",
    "    if p < 0.01:\n",
    "        Rs+=\"*\"\n",
    "\n",
    "    vmin =-1.5\n",
    "    vmax = 2.5\n",
    "    # plot scatter\n",
    "    plt.plot(ts_anom, uth_anom, 'o',color=\"#2E75B6\",mec='#1F3649')\n",
    "    # plot fit\n",
    "    xg = np.linspace(vmin,vmax,50)\n",
    "    yg = intercept+slope*xg\n",
    "    plt.plot(xg, yg,'--',color=\"#C55A11\",lw=2,label=f\"{slope:0.2f}\"+r\"$\\pm$\"+f\"{se:0.2f} %/K\")\n",
    "    plt.ylabel(f\"{region1} \"+r\"$\\Delta$UTH (%)\")\n",
    "    plt.xlabel(f'{region2} '+r\"$\\Delta$T$_{s}$ (K)\")\n",
    "    plt.title(f'R={Rs}')\n",
    "    plt.ylim(-2.5,1.5)\n",
    "    plt.xlim(vmin,vmax)\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2a619-40ad-4dfe-992f-04f387f3b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "def read_gridded_data(filename, report=True):\n",
    "    \"\"\" function to read regular gridded data files and return a Python dictionary.\n",
    "    inputs: filename -> name of file to be read\n",
    "            report   -> boolean flag, if true then a table listing the file contents is printed to screen\n",
    "\n",
    "    outputs: data    -> a python dictionary containing the file contents\n",
    "    \"\"\"\n",
    "    # open the file and map the contents to a netCDF object. Use a test to capture\n",
    "    # any issues with the data file\n",
    "    try:\n",
    "        nc = netCDF4.Dataset(filename,\"r\")\n",
    "    except IOError:\n",
    "        raise\n",
    "    # if report is set to true then prind global attributes\n",
    "    if report == True:\n",
    "        print(f\"Global Attributes For File: {filename}\") # when witing strings, starting them with an 'f' allows you to insert other variables using {}.\n",
    "        print(\"----------------------------------------------------------------------------------------------------\")\n",
    "        print(nc)\n",
    "    # Define a dictionary to hold the contents\n",
    "    data = {} \n",
    "    # loop over the file contents and write\n",
    "    if report == True:\n",
    "        # what variables are in this file?\n",
    "        # printing Aligned Header \n",
    "        print(\"-------------------------------------------------------------------\")        \n",
    "        print(f\"{'Variable Name' : <14} |{'Long Name':<32} |{'tdim, ydim, xdim':>17}\") \n",
    "        print(\"-------------------------------------------------------------------\")\n",
    "    else:\n",
    "        pass # we add a pass so we can close this if statement with an else\n",
    "\n",
    "    for varname in nc.variables.keys():\n",
    "        if report == True:\n",
    "            print(f\"{varname:<14} |{nc[varname].long_name:<32} |{', '.join([str(d) for d in nc[varname].shape]):>17}\")\n",
    "        else:\n",
    "            pass\n",
    "        # write the variable to the dictionary\n",
    "        data[varname] = nc[varname][:] #  the [:] at the end indexes all the data\n",
    "    \n",
    "    # finally close the file\n",
    "    nc.close()\n",
    "\n",
    "    # return the filled data dictionary\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1019052-71f2-404a-8eee-18c1bf76e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#                                                                      #\n",
    "########################################################################\n",
    "def convert_lon(lons,reverse=0):\n",
    "    if reverse == 0:\n",
    "        return np.fmod((lons + 360), 360.0)\n",
    "    else:\n",
    "        return np.fmod((lons + 180), 360.0) - 180\n",
    "\n",
    "########################################################################\n",
    "#                                                                      #\n",
    "########################################################################\n",
    "def create_enso_timeseries(tskin, lats, lons, lsm):\n",
    "\n",
    "    # 1. create 2D lons and lats\n",
    "    lons2d, lats2d = np.meshgrid(lons,lats)\n",
    "\n",
    "    # calculate the weights\n",
    "    wgts = np.cos(np.radians(lats2d))\n",
    "    \n",
    "    ####################################################################\n",
    "    # NOW WE CALCULATE THE SST TIME SERIES FOR DIFFERENT ENSO REGIONS\n",
    "    #\n",
    "    # 2. define ENSO regions\n",
    "    # nino regions [min_x, max_x, min_y, max_y]\n",
    "    # for longitude grid that runs from 0-360\n",
    "    regions={'Nino_1+2':[convert_lon(-90),\n",
    "                         convert_lon(-80),\n",
    "                         -10,0],\n",
    "             'Nino_3':[convert_lon(-150),\n",
    "                       convert_lon(-90),\n",
    "                       -5, 5],\n",
    "             'Nino_3.4':[convert_lon(-170),\n",
    "                        convert_lon(-120),\n",
    "                        -5, 5],\n",
    "             'Nino_4':[convert_lon(160),\n",
    "                   convert_lon(-150),\n",
    "                   -5, 5]}\n",
    "    # 3. calculate gridded tskin anomolies\n",
    "    tdim, ydim, xdim = tskin.shape\n",
    "    enso_ts = {}\n",
    "   \n",
    "    \n",
    "    # loop over each time step and andcalculated weighted mean \n",
    "    for tt, grid in enumerate(tskin):\n",
    "        mdx = tt % 12\n",
    "        for reg in regions:\n",
    "            if tt == 0:\n",
    "                enso_ts[reg] = np.full(tdim, np.nan)\n",
    "            else:\n",
    "                pass\n",
    "            bbox = regions[reg]\n",
    "            find = np.where((lons2d >= bbox[0])&\\\n",
    "                            (lons2d <= bbox[1])&\\\n",
    "                            (lats2d >= bbox[2])&\\\n",
    "                            (lats2d <= bbox[3])&\\\n",
    "                            (lsm == 0))\n",
    "            \n",
    "            wgts = np.cos(np.radians(lats2d[find]))\n",
    "            enso_ts[reg][tt] = np.nansum(grid[find]*wgts)/np.nansum(wgts)\n",
    "            \n",
    "\n",
    "\n",
    "    # calculate weighted mean sst anomoly time series\n",
    "    sst_anomolies = {}\n",
    "    for reg in regions:\n",
    "        sst_anomolies[reg] = np.full(tdim, np.nan)\n",
    "        for mth in range(12):\n",
    "            sst_anomolies[reg][mth::12] = enso_ts[reg][mth::12] - np.nanmean(enso_ts[reg][mth::12])\n",
    "            \n",
    "    return sst_anomolies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12566605-e714-4624-a499-bccb15e617ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#                                                                      #\n",
    "########################################################################\n",
    "def mad(x):\n",
    "    \"\"\" calculate the medain absolute deviation \"\"\"\n",
    "    return np.median(abs(x-np.median(x)))\n",
    "\n",
    "########################################################################\n",
    "#                                                                      #\n",
    "########################################################################\n",
    "def robust_zscore(x):\n",
    "    \"\"\" calculate the robust z scores for a set of data.\"\"\"\n",
    "    return 0.6745*(x - np.median(x))/mad(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5832499-12bb-4c93-a30e-ea1480437f9a",
   "metadata": {},
   "source": [
    "***\n",
    "# define data files to be used\n",
    "- This is all the data we are using in this example notebook. Tou will notcice that some of the filepaths have got a ```../``` infront of them . This tells the computer to go up one level in the file directory before decending into the correct directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f37d8-cfd8-4cb1-991c-7857c293122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1: Anomaly time series \n",
    "anom_ts_filename = \"data/uth_ts_anomaly_timeseries.csv\"\n",
    "\n",
    "# example 2 + 3: calculate ENSO ssts from ERA5 data\n",
    "lsm_filename = \"../PA260X_CourseWorkMaterials/data/pa260x_ecmwf_Land_sea_mask_2.5_x_2.5_1950_2020.nc\"\n",
    "Ts_filelist= [\"../PA260X_CourseWorkMaterials/data/pa260x_ecmwf_Skin_temperature_2.5_x_2.5_1950_1989.nc\",\n",
    "              \"../PA260X_CourseWorkMaterials/data/pa260x_ecmwf_Skin_temperature_2.5_x_2.5_1990_2020.nc\"]\n",
    "\n",
    "# eample 4: filetring data with robust z scores\n",
    "tcwv_filename = \"data/reyk_suominet_gps_tcwv_2008_2020.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53ff47-62c3-4453-a3e4-2c39b303fda6",
   "metadata": {},
   "source": [
    "***\n",
    "# Set out the workflow\n",
    "# Example 1: plot PDF and scatter plot example using previous CW rsults\n",
    "- In this example we will recreate a plot similar to one we saw in the last lecture. I have already exported all the data you need into a csv file.\n",
    "- Lets start by reading that in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab94ec7-5db9-4f7c-9c5b-b9bde514414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. read in the time series\n",
    "anom_data = read_csv(anom_ts_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b2dab-84b7-4ef9-833a-243ff9a375dd",
   "metadata": {},
   "source": [
    "- next list the file contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290ba88-b644-4fda-a737-fff3effe06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the file contents\n",
    "sorted(anom_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7fa210-7492-404a-b5b2-ff6df8d6ba5d",
   "metadata": {},
   "source": [
    "- the next task is to identify the La Nina and El Nino relevant data.\n",
    "- I have included a function to this <span color=\"blue\">**find_enso_periods**</span> to do this. Don't worry if isn't clear, its not something we have covered in workshops or lectures. Later we will resue it to calculate an ENSO phase index that you can use for your course work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad0c22-820d-4531-9a18-e2c79541d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. seperate the data out the enso phases\n",
    "#######################################\n",
    "# detect ENSO phases, we are going to use the ONI (Oceanic Nino Index) \n",
    "smth_win = 3\n",
    "thresh=[-0.5,0.5]\n",
    "nmonths=5\n",
    "\n",
    "# smooth the time series with running average of 2 months\n",
    "smoothed_ts = simple_moving_average(anom_data['sea_surface_temperature_anomaly_enso_3.4'], smth_win)\n",
    "\n",
    "# detect months where the sst anomoly values exceed the\n",
    "# threshold values. This produces 2 array of 1s and 0s\n",
    "lanina = (smoothed_ts < thresh[0]).astype(int)\n",
    "elnino = (smoothed_ts > thresh[1]).astype(int)\n",
    "\n",
    "el_periods, el_max_anom, el_years = find_enso_periods(smoothed_ts,\n",
    "                                                      elnino,\n",
    "                                                      anom_data['frac_year'], \n",
    "                                                      nmonths,\n",
    "                                                      phase='Elnino')\n",
    "\n",
    "la_periods, la_max_anom, la_years = find_enso_periods(smoothed_ts,\n",
    "                                                      lanina,\n",
    "                                                      anom_data['frac_year'], \n",
    "                                                      nmonths,\n",
    "                                                      phase='Lanina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4e684-8872-4707-a859-36256d421330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. seperate out data into el nino and la nina phases\n",
    "la_results_lst = []\n",
    "la_results_uth=[]\n",
    "la_yrs = []\n",
    "\n",
    "# first we loop throughthe identified La Nina events\n",
    "for phase in la_periods:\n",
    "    for fy in phase:\n",
    "        # match the dates to extract the index in the anomaly tiumeseries arrays\n",
    "        idx = abs(anom_data['frac_year']-fy).argmin()\n",
    "        la_results_lst.append(anom_data['smoothed_land_surface_temperature_anomaly_tropics'][idx])\n",
    "        la_results_uth.append(anom_data['smoothed_uth_anomaly_tropics_land'][idx])\n",
    "        la_yrs.append(int(fy))\n",
    "        \n",
    "\n",
    "el_results_lst = []\n",
    "el_results_uth= []\n",
    "el_yrs = []\n",
    "for phase in el_periods:\n",
    "    for fy in phase:\n",
    "        # match the dates to extract the index in the anomaly tiumeseries arrays\n",
    "        idx = abs(anom_data['frac_year']-fy).argmin()\n",
    "        el_results_lst.append(anom_data['smoothed_land_surface_temperature_anomaly_tropics'][idx])\n",
    "        el_results_uth.append(anom_data['smoothed_uth_anomaly_tropics_land'][idx])\n",
    "        el_yrs.append(int(fy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7befbd6-0a1d-4f17-8f59-a199e39f8227",
   "metadata": {},
   "source": [
    "- then we can split the identified La Nina and El Nino data into rpe and after 2001 using a ```np.where``` statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e05739-195c-47d0-870e-e444d2f90c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. create index values for La Nina and El Nino phases, pre and post 2001\n",
    "##########################################################################\n",
    "la_pre2000 = np.where(np.array(la_yrs) < 2001)\n",
    "la_aft2000 = np.where(np.array(la_yrs) >= 2001)\n",
    "\n",
    "el_pre2000 = np.where(np.array(el_yrs) < 2001)\n",
    "el_aft2000 = np.where(np.array(el_yrs) >= 2001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b32641d-b0b1-436c-b861-799e900f02b2",
   "metadata": {},
   "source": [
    "- calculate PDFs reusing code from the first workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3615c9c6-cdf9-4192-aeba-b21423452d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Calculate PDFs\n",
    "#########################################################################################################\n",
    "# LST\n",
    "pre_la_lst_pdf, xvals_lst = calculate_probDensFunc(np.array(la_results_lst)[la_pre2000], -2, 5, nbins=80)\n",
    "aft_la_lst_pdf, xvals_lst = calculate_probDensFunc(np.array(la_results_lst)[la_aft2000], -2, 5, nbins=80)\n",
    "pre_el_lst_pdf, xvals_lst = calculate_probDensFunc(np.array(el_results_lst)[el_pre2000], -2, 5, nbins=80)\n",
    "aft_el_lst_pdf, xvals_lst = calculate_probDensFunc(np.array(el_results_lst)[el_aft2000], -2, 5, nbins=80)\n",
    "\n",
    "# UTH\n",
    "pre_la_uth_pdf, xvals_uth = calculate_probDensFunc(np.array(la_results_uth)[la_pre2000], -5, 3, nbins=80)\n",
    "aft_la_uth_pdf, xvals_uth = calculate_probDensFunc(np.array(la_results_uth)[la_aft2000], -5, 3, nbins=80)\n",
    "pre_el_uth_pdf, xvals_uth = calculate_probDensFunc(np.array(el_results_uth)[el_pre2000], -5, 3, nbins=80)\n",
    "aft_el_uth_pdf, xvals_uth = calculate_probDensFunc(np.array(el_results_uth)[el_aft2000], -5, 3, nbins=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58920620-02b8-4a4c-a878-9b4f0087cf3e",
   "metadata": {},
   "source": [
    "- finally plot the results\n",
    "- You will notice we now have functions to plot the data and our code below is tidier and easy to follow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f0575a-d789-4639-bfc0-11606d9755b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. plot the results\n",
    "############################################################################################################\n",
    "\n",
    "plt.figure(figsize=(13,4),dpi=300)\n",
    "\n",
    "plt.subplot(131)\n",
    "plot_enso_phase_pdfs(xvals_lst, pre_el_lst_pdf, aft_el_lst_pdf, pre_la_lst_pdf, aft_la_lst_pdf, 'lst')\n",
    "plt.title(r\"LST Anomalies over the Tropics\")\n",
    "\n",
    "\n",
    "plt.subplot(132)\n",
    "plot_uth_vs_Ts(anom_data['smoothed_land_surface_temperature_anomaly_tropics'], \n",
    "               anom_data['smoothed_uth_anomaly_tropics_land'],'(Tropical Land)', '(Tropical Land)')\n",
    "\n",
    "plt.subplot(133)\n",
    "plot_enso_phase_pdfs(xvals_uth, pre_el_uth_pdf, aft_el_uth_pdf, pre_la_uth_pdf, aft_la_uth_pdf, 'uth')\n",
    "plt.title(r\"UTH Anomalies Over the Tropics\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(\"uth_results_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eddf0fd-34a5-4038-9cd6-bb65f524802a",
   "metadata": {},
   "source": [
    "***\n",
    "# Example 2: caluclate ENSO 3.4 SST anomalies from coursework data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e264af-0620-4ad4-acbf-52a4cd959a37",
   "metadata": {},
   "source": [
    "- In this example we are going to calculate the SST anomolies for the 4 ENSO regions usingthe coursework surface temeprature dataset. Lets start by reading in the Land Sea Mask (LSM) and the surface skin temperature datasets. Like with all NetCDF files we use our <span style=\"color:blue\">**read_gridded_data()**</span>. Lets start with the LSM file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521bb85d-ce83-4510-bcf6-7bb6cea125f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsm_data = read_gridded_data(lsm_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c7b6a-c2d8-4178-ba3e-5edb4601982f",
   "metadata": {},
   "source": [
    "- with the report flag set to <span style=\"color:green\">**True**</span> we get the information on the file contents. We can see that there is just the LSM, longitude, and latitude information. For the skin temperature we have 2 files which we will need to combine. Lets read in the first file in the list and see what is inside: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3ff35-e3fd-40ae-b5d6-79bcae2e5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data = read_gridded_data(Ts_filelist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb58bf-b0e9-4b52-a5fb-9dc1d9c9ed24",
   "metadata": {},
   "source": [
    "- now we can see all the variables we can write the code to join up the time series. We have 2 variables that need to be joined and they are ```skt``` (with is 3D) and ```time``` (which is 1D).\n",
    "- Below I have edit some code from the second computer workshop where we stacked AVHRR reflectance data.\n",
    "- for the skin temeprature ```skt``` we use  ```np.vstack``` to join the data in the time dimension, and for ```time``` we just use ```np.append``` to append the arrays together  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448d0bf-419b-4810-8f4b-00bc54ee7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let now loop over each file, read the contents, and stack the results\n",
    "for ii, fname in enumerate(sorted(Ts_filelist)):\n",
    "    # because we can iterate overlists in Python, we canuse the enumerate() \n",
    "    # method to give us the positional index at the same time (e.g. 0,1,2,3..).\n",
    "    # We have also use the sorted() method to force the list into chronological\n",
    "    # order - which happens because the filenames have been formatted with the \n",
    "    # date in YYYYMMDD at the end of the filename.  \n",
    "    \n",
    "    if ii == 0:\n",
    "        # for the first file we assign it to the dictionary avhrr_data with Ch1 and Ch2 variables \n",
    "        # dimensions of (1,900,1800)\n",
    "        ts_data = read_gridded_data(fname, report=False)\n",
    "    else:\n",
    "        # for every other file we stack the contents of Ch1 and Ch2 on top such that the dimensions\n",
    "        # avhrr_data[Ch1] and avhrr_data[Ch2] are (2,900,1800) when the second file is read in, \n",
    "        # (3,900,1800) when the third is read in and so on until the final dimensions are (10,900,1800)\n",
    "        \n",
    "        tmp = read_gridded_data(fname, report=False) # hold output in a temporary dictionarry\n",
    "        ts_data['skt'] = np.vstack([ts_data['skt'], tmp['skt']])\n",
    "        ts_data['time'] = np.append(ts_data['time'], tmp['time'])\n",
    "\n",
    "        # delete the temporary dictionary\n",
    "        del tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f764d9-9b20-4455-a109-16f18a75a196",
   "metadata": {},
   "source": [
    "- now we have the data we can calculate the SST anomolies for the ENSO regions (we haven't covered this code  previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950c34a-e6c8-4b54-bb38-21f8e06a0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enso_ssts = create_enso_timeseries(ts_data['skt'], ts_data['latitude'], ts_data['longitude'], lsm_data['lsm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858b700-1b57-4cf1-90ba-bec357a9e5b1",
   "metadata": {},
   "source": [
    "- Lets plot the data so we can examine it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228bfdf-371a-4884-816d-cec29be5bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple four panel plot of ENSO region SST anomalies\n",
    "plt.figure(figsize=(7,4),dpi=200)\n",
    "for pp, enso_reg in enumerate(enso_ssts.keys()):\n",
    "    plt.subplot(2,2,pp+1)\n",
    "    x = ts_data['time']\n",
    "    y = np.full(x.size,0)\n",
    "    plt.plot(x,y,'--r')\n",
    "    plt.plot(x, enso_ssts[enso_reg])\n",
    "    plt.title(enso_reg)\n",
    "    plt.ylim(-3,5)\n",
    "    plt.xlim(1950,2021)\n",
    "    plt.ylabel(r\"$\\Delta$SST [K]\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b91922-bd7d-4ef3-b855-c39f6f9d092c",
   "metadata": {},
   "source": [
    "- Next we can determine which monthly time steps are in either the El Nino, neutral, or La Nina phase. For this we apply the Oceanic Nino Index (ONI) technique. Anomalies are calculated for specific regions for a specific reference period so we can then apply a 2 month moving average filter to smooth the data. Threshold is then set to $\\pm$0.5 K.\n",
    "- We haven't covered this before so don't worry if it does not seem clear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2798c6e2-1ea4-4221-a19f-c3cdb0e17355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. seperate the data out the enso phases\n",
    "#######################################\n",
    "# detect ENSO phases, we are going to use the ONI (Oceanic Nino Index) \n",
    "smth_win = 3\n",
    "thresh=[-0.5,0.5]\n",
    "nmonths=5\n",
    "\n",
    "# smooth the time series with running average of 2 months\n",
    "smoothed_ts = simple_moving_average(enso_ssts['Nino_3.4'], smth_win)\n",
    "\n",
    "# detect months where the sst anomoly values exceed the\n",
    "# threshold values. This produces 2 array of 1s and 0s\n",
    "lanina = (smoothed_ts < thresh[0]).astype(int)\n",
    "elnino = (smoothed_ts > thresh[1]).astype(int)\n",
    "\n",
    "el_periods, el_max_anom, el_years = find_enso_periods(smoothed_ts,\n",
    "                                                      elnino,\n",
    "                                                      ts_data['time'], \n",
    "                                                      nmonths,\n",
    "                                                      phase='Elnino')\n",
    "\n",
    "la_periods, la_max_anom, la_years = find_enso_periods(smoothed_ts,\n",
    "                                                      lanina,\n",
    "                                                      ts_data['time'], \n",
    "                                                      nmonths,\n",
    "                                                      phase='Lanina')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef19815-3593-42d0-bf73-b580a4203910",
   "metadata": {},
   "source": [
    "- Lets make an index array that can be used later to filter other timeseries calcuylated from the coursework datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67793242-fa67-4dba-b435-fabc375e3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "enso_phase_index = np.full(ts_data['time'].size,0)\n",
    "enso_phase_index[lanina == 1] = -1\n",
    "enso_phase_index[elnino == 1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef6267e-a6e8-4224-abab-2deb35fc9670",
   "metadata": {},
   "source": [
    "- lets make a plot to illustrate the enso pahse index array we have just made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004d218-65b2-41a4-8419-198e38c0dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,2),dpi=200)\n",
    "plt.fill_between(x, enso_phase_index, where=enso_phase_index >=0, facecolor=\"#C55A11\",interpolate=True)\n",
    "plt.fill_between(x, enso_phase_index, where=enso_phase_index < 0, facecolor=\"#2E75B6\",interpolate=True)\n",
    "plt.plot(x, enso_phase_index,color='#1F3649')\n",
    "plt.title('Enso Phase Index')\n",
    "plt.ylim(-1.1,1.1)\n",
    "plt.xlim(1950,2021)\n",
    "plt.ylabel(r\"Index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceabd5f-ee51-4581-955e-49171d605312",
   "metadata": {},
   "source": [
    "- The code below is a slightly modified version of the plotting code in the cell above and we visualise our SST anomolies from the Nino 3.4 region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ad84a-f8fe-4b47-bdd1-42ca74e18de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,2),dpi=200)\n",
    "plt.fill_between(x, smoothed_ts, where=smoothed_ts>=0, facecolor=\"r\",interpolate=True)\n",
    "plt.fill_between(x, smoothed_ts, where=smoothed_ts < 0, facecolor=\"b\",interpolate=True)\n",
    "plt.plot(x,smoothed_ts,color='#1F3649')\n",
    "plt.title('Nino 3.4')\n",
    "plt.ylim(-3, 3)\n",
    "plt.xlim(1950,2021)\n",
    "plt.ylabel(r\"$\\Delta$SST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0289cc-6f61-46fd-b540-592e48134fc6",
   "metadata": {},
   "source": [
    "## Now We can save this for use another time! We reuse a piece of code to write csv files with a small update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dabb4ad-d0e6-491c-9eab-32dccab9458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# This our code to save the outputs\n",
    "# we open a file called uth_anomoly_timeseries.csv and assign it to a file object we have named fobj\n",
    "# we can the loop over each time step and write the results to file\n",
    "with open(\"enso_3.4_sst_anomaly.csv\", \"w\") as fobj:\n",
    "    # first we write a header, the \\n tells the computer to start a new line\n",
    "    fobj.write(\"Frac_Year, SST_anomaly_(NINO_3.4), enso_phase_index\\n\")\n",
    "    # here we use zip in order to interate over all arrays at the same time\n",
    "    for yr, v1, v2 in zip(ts_data['time'], enso_ssts['Nino_3.4'], enso_phase_index):\n",
    "        # we create a f string and write it to file\n",
    "        fobj.write(f\"{yr:0.3f}, {v1:0.2f}, {v2:0.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92606f8e-0a9b-46f6-91a5-06404da2bb23",
   "metadata": {},
   "source": [
    "***\n",
    "# Example 3: Correlation to climate indices\n",
    "- In this example we will calculate the correlation to climate indicies and plot a map\n",
    "- We will look at the correlation between our smoothed Nino 3.4 SST anomolies and surface skin temperature\n",
    "- we will use the ```stats.pearsonr``` from scipy for this task. You could do something similar to calculate trends using ```stats.linregress```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5f092-c294-46ac-8287-bbe028033f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our dims\n",
    "tdim, ydim, xdim = ts_data['skt'].shape\n",
    "\n",
    "# set up an array to hold our results\n",
    "corr_to_enso = np.full((ydim,xdim),np.nan)\n",
    "sig_corr = np.full((ydim,xdim),np.nan)\n",
    "\n",
    "# now we loop over the y and x dimensions and calculate the correlation between the skin temperature (skt) ans the ENSO SST anomolies\n",
    "for j in range(ydim):\n",
    "    for i in range(xdim):\n",
    "        # calculate correlation for grid cell (j,i)\n",
    "        res = stats.pearsonr(ts_data['skt'][:,j,i], smoothed_ts)\n",
    "        # write correlation result to array\n",
    "        corr_to_enso[j,i] = res.statistic\n",
    "        # if significant mark the poitionin the sig_corr array with a 1\n",
    "        if res.pvalue < 0.05:\n",
    "            sig_corr[j,i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869bc03c-32eb-4b10-b14e-f5693661da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.pcolormesh(ts_data['longitude'], ts_data['latitude'], corr_to_enso, vmin=-1, vmax=1,cmap=plt.get_cmap('PiYG', 16))\n",
    "plt.colorbar()\n",
    "plt.contour(ts_data['longitude'], ts_data['latitude'],lsm_data['lsm'],levels=[0,1],cmap='Greys_r')\n",
    "plt.title(\"Correlation between ERA5 skin temperature and Nino3.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70533dd0-f789-4f09-a011-68ed35bb812d",
   "metadata": {},
   "source": [
    "- Now we can make a nice plot of the results lets add the statistcial significance to the plot with stippling.\n",
    "- Essentially we will plot a small dot if the data is statistciall significant. The results of this are held in the ```sig_corr``` array, which eith has a 1 or NaN entry for each grid cell.\n",
    "- We can use the ```np.ma.masked_invalid``` function to mask the NaN values in the ```sig_corr``` array. If we then create 2D arrays of longitude and latitude we can apply the same mask to them.\n",
    "- Finally, we use the ```.compressed()``` method onour newly masked 2D arrays of long itude and latitude to turn them back to 1D and just polt the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33a413-5db9-4689-b2b5-b55f099fc63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2D arrays of longigude and latitude\n",
    "x2d, y2d = np.meshgrid(ts_data['longitude'], ts_data['latitude'])\n",
    "# mask the sig_corr array\n",
    "sig_corr = np.ma.masked_invalid(sig_corr)\n",
    "# apply the mask to the 2D lons and lats, then compress\n",
    "x2d = np.ma.masked_array(x2d,mask=sig_corr.mask).compressed()\n",
    "y2d = np.ma.masked_array(y2d,mask=sig_corr.mask).compressed()\n",
    "\n",
    "# plot the same figure from before\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.pcolormesh(ts_data['longitude'], ts_data['latitude'], corr_to_enso, vmin=-1, vmax=1,cmap=plt.get_cmap('PiYG', 16))\n",
    "plt.colorbar()\n",
    "plt.contour(ts_data['longitude'], ts_data['latitude'],lsm_data['lsm'],levels=[0,1],cmap='Greys_r')\n",
    "plt.title(\"Correlation between ERA5 skin temperature and Nino3.4\")\n",
    "# now add the stippling to indicate areas of statisitical significance in the correlations\n",
    "plt.plot(x2d,y2d,'.',ms=1,color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c015889-3ce7-40ea-aeb4-2ce8c9cca4f6",
   "metadata": {},
   "source": [
    "- We can easily apply the same process to skin temperature trends.\n",
    "- First we calculate the anomalies relative to the 1950-1990 average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e7f7f-c392-4cd8-b3de-5b88c60a9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up an array to hold our results\n",
    "skt_trend = np.full((ydim,xdim),np.nan)\n",
    "sig_corr = np.full((ydim,xdim),np.nan)\n",
    "\n",
    "# calculate Tskin anomalies relative to 1950-1980\n",
    "anom = np.full((tdim, ydim,xdim),np.nan)\n",
    "mkr = np.where((ts_data['time'] >= 1950)&(ts_data['time'] < 1990))\n",
    "for mth in range(12):\n",
    "    anom[mth::12,:,:] = ts_data['skt'][mth::12,:,:] - np.mean(ts_data['skt'][mkr[0],:,:][mth::12,:,:],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45680e-9e6f-49f5-8c5c-2da23b2494b6",
   "metadata": {},
   "source": [
    "- we then just repeat the same code structure and preplace the pearson correlation function from scipy with linregress\n",
    "- the other difference is we scale the slope from per month to per decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22947b-5964-46f0-841f-6d775cacbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we loop over the y and x dimensions and calculate the linear fit between the skin temperature and time\n",
    "for j in range(ydim):\n",
    "    for i in range(xdim):\n",
    "        # calculate correlation for grid cell (j,i)\n",
    "        res = stats.linregress(ts_data['time'],anom[:,j,i])\n",
    "        # write slope result to array, scale fromper month to per decade\n",
    "        skt_trend[j,i] = res.slope*120\n",
    "        # if significant mark the poitionin the sig_corr array with a 1\n",
    "        if res.pvalue < 0.005:\n",
    "            sig_corr[j,i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96285159-f5be-4f54-8a45-0a71d42f1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2D arrays of longigude and latitude\n",
    "x2d, y2d = np.meshgrid(ts_data['longitude'], ts_data['latitude'])\n",
    "# mask the sig_corr array\n",
    "sig_corr = np.ma.masked_invalid(sig_corr)\n",
    "# apply the mask to the 2D lons and lats, then compress\n",
    "x2d = np.ma.masked_array(x2d,mask=sig_corr.mask).compressed()\n",
    "y2d = np.ma.masked_array(y2d,mask=sig_corr.mask).compressed()\n",
    "\n",
    "# plot the same figure from before\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.pcolormesh(ts_data['longitude'], ts_data['latitude'], skt_trend, vmin=-6, vmax=6,cmap=plt.get_cmap('coolwarm', 12))\n",
    "plt.colorbar(extend='both')\n",
    "plt.contour(ts_data['longitude'], ts_data['latitude'],lsm_data['lsm'],levels=[0,1],cmap='Greys_r')\n",
    "plt.title(\"ERA5 skin Temperature Trend (K/decade)\")\n",
    "# now add the stippling to indicate areas of statisitical significance of the trends\n",
    "plt.plot(x2d,y2d,'.',ms=1,color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25d486-b197-4cf9-be77-85ac6942d4e5",
   "metadata": {},
   "source": [
    "### What can we infer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240bd173-d580-4168-9d50-9b5bd736bb40",
   "metadata": {},
   "source": [
    "***\n",
    "# Example 4: robust z scores\n",
    "![](img/suominet_info.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f2b43a-8af8-478e-b335-5a49df21cb28",
   "metadata": {},
   "source": [
    " - Lets start by reading in the data. The file is in csv format so we can reuse our <span style=\"color:blue\">**read_csv()**</span> funtion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89990bd8-b456-4c84-afb4-f9344cfb6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. read in the time series\n",
    "gps_data = read_csv(tcwv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ea968-a24f-443e-bed4-2957c56b7cb0",
   "metadata": {},
   "source": [
    "### Next we can create a simple time series to look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce085d24-778b-4c98-8431-c37f3cadf19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a simple time series plot of the data\n",
    "plt.figure(figsize=(12,3),dpi=200)\n",
    "plt.plot(gps_data['frac_year'], gps_data['tcwv'],'o',ms=1)\n",
    "plt.ylabel(r\"TCWV (kg/m$^{2}$)\")\n",
    "plt.xlim(2008,2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f6bb3-e8d6-4ec8-af1f-9c45c394ad8f",
   "metadata": {},
   "source": [
    "- In this example we obsevre a shift in the data between late 2016 and mid-2018, which appears to be biased high releative to the rest of the data. The question is how can we tes that this is true and filter the data if required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d402414f-2d48-48a1-9030-5a12b40483e1",
   "metadata": {},
   "source": [
    "## The robsut z score\n",
    "\n",
    "- A **robust Z-score** is a modified version of the standard Z-score that helps detect outliers while being less sensitive to extreme values and skewed distributions. Instead of using the mean and standard deviation, robust Z-scores are calculated using the **median** and **median absolute deviation (MAD)**:\n",
    "\n",
    "# $z_{\\text{robust}} = \\frac{0.6745(x - median)}{MAD}$\n",
    "\n",
    "- here MAD is a robust measure of how spread out a set of data is about the median:\n",
    "\n",
    "### $MAD = medain(|x - median(x)|)$ \n",
    "\n",
    "- The factor 0.6745 ensures consistency with the standard deviation for normally distributed data. Values of $( |Z_{\\text{robust}}| > 3 )$ typically indicate potential outliers. Robust Z-scores are particularly useful when dealing with non-normal data or datasets containing extreme values, as they provide a more reliable way to identify outliers than traditional Z-scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd938a0-f4e2-467f-acca-ca94b0a6521e",
   "metadata": {},
   "source": [
    "- To represent these as functions we would use the following python code:\n",
    "\n",
    "```python\n",
    "    def mad(x):\n",
    "        \"\"\" calculate the medain absolute deviation \"\"\"\n",
    "        return np.median(abs(x-np.median(x)))\n",
    "    \n",
    "    def robust_zscore(x):\n",
    "        \"\"\" calculate the robust z scores for a set of data.\"\"\"\n",
    "        return 0.6745*(x - np.median(x))/mad(x)\n",
    "\n",
    "```\n",
    "- These have already been included in the functions above, so we can have a go at filtering the data. First we pass our TCWV data to the  <span style=\"color:blue\">**robust _zscore()**</span> function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65664dd-8881-4d8f-90bc-7d9b940656e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the z scores\n",
    "z = robust_zscore( gps_data['tcwv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4bfc2e-1be1-4fd7-a003-aa93578fccf6",
   "metadata": {},
   "source": [
    "- Then we can make a simple plot to look at our results and see if we have a cluster of data outside $\\pm$3 threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2970cf-8e14-44f5-93e4-f7999e448372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of absolute values \n",
    "h = plt.hist(abs(z), range=(0,7),bins=100) # here we use the python function abs() which converts all values to absolute o positive values\n",
    "ymax = np.ceil(h[0].max()/10)*10\n",
    "plt.plot([3,3],[0,ymax],'--r')\n",
    "plt.xlim(0,7)\n",
    "plt.ylim(0,ymax)\n",
    "plt.ylabel(\"Number of points\")\n",
    "plt.xlabel(\"Z Score \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3928701-ada1-4787-8df5-4bb81e3b176b",
   "metadata": {},
   "source": [
    "- we observe a population of the data to the righ hand side of the red dashed line on the plot. Therefore, there is a good chance this is our outlier data. To find outr we write a simple ```np.where``` statement to identify the z scores within the $\\pm$3 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31510c4-ea1c-4868-9e32-9765ae73f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "find = np.where(abs(z) <3) # here we use the python function abs() which converts all values to absolute o positive values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6a757-7110-4521-8b11-7762348ed43a",
   "metadata": {},
   "source": [
    "- Finally lets plot the original data in a light grey and then over plot the filtered TCWV data over the top and see how well it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38215902-8e38-4288-b241-5e38857aa4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a simple time series plot of the data\n",
    "plt.figure(figsize=(12,3),dpi=200)\n",
    "plt.plot(gps_data['frac_year'], gps_data['tcwv'],'o', color='lightgrey',ms=1,alpha=0.3, label='unfiltered')\n",
    "plt.plot(gps_data['frac_year'][find], gps_data['tcwv'][find],'o',ms=1,label='filtered')\n",
    "plt.ylabel(r\"TCWV (kg/m$^{2}$)\")\n",
    "plt.xlim(2008,2020)\n",
    "plt.legend(loc=2,markerscale=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57a96a-07cb-4ef3-9e62-7befbd48017b",
   "metadata": {},
   "source": [
    "### How well has this worked?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377faa06-b176-46fa-a223-afdcb8ed249e",
   "metadata": {},
   "source": [
    "***\n",
    "# Bonus examples: Climate indicies\n",
    "- You may want to use a different climate index for you analysis (e.g. qbo for stratospheric temperature)\n",
    "- In the data folder there is a file called ```climate_indicies.csv``` with ENSO 3.4, NAO, QBO, and PDO\n",
    "- Below is an example where i have read them in and made simple time series plots \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078bed6-880b-4b3e-9a86-d288118c035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. read in the time series\n",
    "clim_inds = read_csv(\"data/climate_indicies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262ef08-bb33-43b7-bb87-1f66b2e6a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,8),dpi=200)\n",
    "for ii, ci in enumerate(['qbo', 'pdo', 'nao', 'enso3.4']):\n",
    "    plt.subplot(4,1,ii+1)\n",
    "    plt.fill_between(clim_inds['frac_year'], clim_inds[ci], where=clim_inds[ci]>= 0, facecolor=\"r\",interpolate=True)\n",
    "    plt.fill_between(clim_inds['frac_year'], clim_inds[ci], where=clim_inds[ci] < 0, facecolor=\"b\",interpolate=True)\n",
    "    plt.plot(clim_inds['frac_year'],clim_inds[ci],color='#1F3649')\n",
    "    plt.title(ci.upper()) # we can capitalise the string with the .upper() method\n",
    "    if ci == 'qbo':\n",
    "        plt.ylim(-35, 35)\n",
    "    else:\n",
    "        plt.ylim(-3.5,3.5)\n",
    "    plt.xlim(1950,2021)\n",
    "    plt.ylabel(r\"Index\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6878f4f-e629-470e-9947-3f812dd560c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
